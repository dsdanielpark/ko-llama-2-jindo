{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer \n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "trainer = transformers.Trainer( \n",
    "    model=model, \n",
    "    train_dataset=data[ \"train\" ], \n",
    "    args=transformers.TrainingArguments( \n",
    "        per_device_train_batch_size= 1 , \n",
    "        gradient_accumulation_steps= 8 , \n",
    "        warmup_steps= 2 , \n",
    "        max_steps= 20 , \n",
    "        learning_rate= 2e-4 , \n",
    "        fp16= True , \n",
    "        logging_steps= 1 , \n",
    "        output_dir= \"outputs\" , \n",
    "        optim=\"paged_adamw_8bit\"\n",
    "     ), \n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm= False ), \n",
    ") \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ask not what your country\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
