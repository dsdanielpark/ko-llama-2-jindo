{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gen from https://huggingface.co/datasets/junelee/sharegpt_deepl_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "def extract_values(json_data_1, json_data_2):\n",
    "    values_list = []\n",
    "    \n",
    "    for item_1, item_2 in zip(json_data_1, json_data_2):\n",
    "        \n",
    "        conversations_1 = item_1.get(\"conversations\", [])\n",
    "        conversations_2 = item_2.get(\"conversations\", [])\n",
    "        \n",
    "        if len(conversations_1) != len(conversations_2):\n",
    "            continue\n",
    "        \n",
    "        for conversation_1, conversation_2 in zip(conversations_1, conversations_2):\n",
    "            value_1 = conversation_1.get(\"value\")\n",
    "            value_2 = conversation_2.get(\"value\")\n",
    "            \n",
    "            temp_dict = {} \n",
    "            \n",
    "            temp_dict['instruction'] = \"translate the following into korean\"\n",
    "            temp_dict['input'] = value_1\n",
    "            temp_dict['output'] = value_2\n",
    "            \n",
    "            if value_1 and value_2:\n",
    "                values_list.append(temp_dict)\n",
    "    \n",
    "    return values_list\n",
    "\n",
    "def process_json_files(file_path_1, file_path_2, chunk_size, output_filepath):\n",
    "    with open(file_path_1) as file:\n",
    "        data_1 = json.load(file)\n",
    "\n",
    "    with open(file_path_2) as file:\n",
    "        data_2 = json.load(file)\n",
    "\n",
    "    data_1_chunks = [data_1[i:i+chunk_size] for i in range(0, len(data_1), chunk_size)]\n",
    "    data_2_chunks = [data_2[i:i+chunk_size] for i in range(0, len(data_2), chunk_size)]\n",
    "\n",
    "    values = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for chunk_1, chunk_2 in zip(data_1_chunks, data_2_chunks):\n",
    "            future = executor.submit(extract_values, chunk_1, chunk_2)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            values.extend(future.result())\n",
    "\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(values, file, ensure_ascii=False,  indent=4, separators=(',', ':')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = r\"C:\\Users\\parkm\\Desktop\\llm\\ko-alpaca-lingo\\data\\dbdu\\ShareGPT-74k-ko\\original_shargpt.json\"\n",
    "file_path_2 = r\"C:\\Users\\parkm\\Desktop\\llm\\ko-alpaca-lingo\\data\\dbdu\\ShareGPT-74k-ko\\ko_sharegpt.json\"\n",
    "chunk_size = 9000000\n",
    "output_filepath = \"ko_shargpt_deepl_translate_v1.json\"\n",
    "process_json_files(file_path_1, file_path_2, chunk_size, output_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utilfunction import find_path\n",
    "import json\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, input_file_path):\n",
    "        with open(input_file_path) as f:\n",
    "            self.data = json.load(f)\n",
    "        self.excluded_data = []\n",
    "\n",
    "    def step_completion(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            print(f\"{func.__name__} completed successfully in {end_time - start_time:.2f} seconds.\")\n",
    "            return result\n",
    "        return wrapper\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def remove_specific_words(data):\n",
    "        word_set = {'sure!', 'great!', 'Certainly!', \"Sure!\", \"Great!\", \"Great, \", \"great, \",\n",
    "                    \"sure, \", \"Sure, \", \"Sure! \"}\n",
    "        return [d for d in data if not any(word in d['input'] for word in word_set)]\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def remove_short_fields(data):\n",
    "        try:\n",
    "            return [d for d in data if (input_len := len(d['input'])) > 4 and (output_len := len(d['output'])) > 4]\n",
    "        except Exception as e:\n",
    "            print(f\"Error at remove_short_fields {e} \\n Finished unsuccessfully.\")\n",
    "            return data\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def replace_sure_translation(data):\n",
    "        for d in data:\n",
    "            try:\n",
    "                d['output'] = re.sub(r'\\b물론,\\b', '물론이죠.', d['output'])\n",
    "                d['output'] = re.sub(r'\\b확실히,\\b', '', d['output'])\n",
    "                d['output'] = re.sub(r'\\b예,\\b', '네.', d['output'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error at replace_sure_translation {e}\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def delete_error_korean_prefix(data):\n",
    "        for d in data:\n",
    "            try:\n",
    "                d['output'] = re.sub(r'^(은 |는 )', '', d['output'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error at delete_error_korean_prefix {e}\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def replace_output_prefix(data):\n",
    "        prefix_set = {\"을 \", \"를 \", \"이 \", \"가 \", \"h\", \"은 \", \"는 \", \"에 \", \"으 \", \"의\", \"예, \", \"^[A-Za-z] \", \"^[ㄱ-ㅎㅏ-ㅣ가-힣] \", \"^[0-9] \", \".\", \",\"}\n",
    "        exclued_data = []\n",
    "\n",
    "        for d in data:\n",
    "            try:\n",
    "                output_text = d['output']\n",
    "                if output_text.startswith(tuple(prefix_set)):\n",
    "                    exclued_data.append(d)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at replace_output_prefix {e}\")\n",
    "            \n",
    "        return exclued_data\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def do_not_translate_code_snippet(data):\n",
    "        for d in data:\n",
    "            try:\n",
    "                input_text = d['input']\n",
    "                output_text = d['output']\n",
    "\n",
    "                if '```' in input_text and '```' in output_text:\n",
    "                    start_index = input_text.find('```') + 3\n",
    "                    end_index = input_text.find('```', start_index)\n",
    "                    replace_text = input_text[start_index:end_index]\n",
    "\n",
    "                    d['output'] = output_text.replace('```', f' ```{replace_text}```')\n",
    "            except Exception as e:\n",
    "                print(f\"Error at do_not_translate_code_snippet {e}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def remove_duplicates(data):\n",
    "        unique_data = []\n",
    "        seen_inputs = set()\n",
    "        seen_outputs = set()\n",
    "        for d in data:\n",
    "            input_value = d[\"input\"]\n",
    "            output_value = d[\"output\"]\n",
    "            if (input_value, output_value) not in seen_inputs and \\\n",
    "                    (input_value, output_value) not in seen_outputs and \\\n",
    "                    input_value != output_value:\n",
    "                seen_inputs.add((input_value, output_value))\n",
    "                seen_outputs.add((input_value, output_value))\n",
    "                unique_data.append(d)\n",
    "        return unique_data\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def remove_deletion_and_addition(data):\n",
    "        for d in data:\n",
    "            input_value = d[\"input\"]\n",
    "            output_value = d[\"output\"]\n",
    "\n",
    "            input_words = input_value.split()\n",
    "            output_words = output_value.split()\n",
    "            try:\n",
    "                if len(output_words[0]) > 1 and len(input_words[0]) > 2:\n",
    "                    if len(set(output_words[0].lower()) - set(input_words[0].lower())) < 2 and \\\n",
    "                            input_words[0][1].lower() == output_words[0][0].lower() and \\\n",
    "                            input_words[0][2].lower() == output_words[0][1].lower():\n",
    "                        output_words[0] = input_words[0]\n",
    "                        output_value = \" \".join(output_words)\n",
    "                        d[\"output\"] = output_value\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            if output_words[0] == \"물론,\":\n",
    "                output_words[0] = \"물론이죠. \"\n",
    "                output_value = \" \".join(output_words)\n",
    "                d[\"output\"] = output_value\n",
    "\n",
    "            if len(output_words[0]) == 1 and output_words[0].isalpha() and output_words[0].isascii() and output_words[0].lower() != \"a\":\n",
    "                output_words[0] = \"\"\n",
    "                output_value = \" \".join(output_words)\n",
    "                d[\"output\"] = output_value\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_list(data):\n",
    "        flattened_list = []\n",
    "        for sublist in data:\n",
    "            if isinstance(sublist, list):\n",
    "                flattened_list.extend(DataProcessor.flatten_list(sublist))\n",
    "            else:\n",
    "                flattened_list.append(sublist)\n",
    "        return flattened_list\n",
    "\n",
    "    @staticmethod\n",
    "    @step_completion\n",
    "    def write_to_file(data, file_path):\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "\n",
    "    @step_completion\n",
    "    def process_json_file(self, steps, output_file_path, dummy_file_path):\n",
    "        for step in steps:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                if 'step1' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_specific_words, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step2' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_short_fields, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step3' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.replace_sure_translation, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step4' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.delete_error_korean_prefix, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step5' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.do_not_translate_code_snippet, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step6' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_duplicates, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step7' in step:\n",
    "                    self.excluded_data = list(executor.map(self.replace_output_prefix, [self.data]))\n",
    "                    self.excluded_data = DataProcessor.flatten_list(self.excluded_data)\n",
    "                    self.data = [d for d in self.data if d not in self.excluded_data]\n",
    "\n",
    "                if 'step8' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_deletion_and_addition, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "        self.data = DataProcessor.flatten_list(self.data)\n",
    "        self.write_to_file(self.data, output_file_path)\n",
    "        self.write_to_file(self.excluded_data, dummy_file_path)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def static_process_json_file(steps, input_file_path, output_file_path, dummy_file_path):\n",
    "        print(f'Qued steps: {steps}')\n",
    "        with open(input_file_path) as f:\n",
    "            data = json.load(f)\n",
    "        excluded_data = []\n",
    "        \n",
    "        for step in steps:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                if 'step1' in step:\n",
    "                    data = list(executor.map(DataProcessor.remove_specific_words, [data]))\n",
    "                    data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                if 'step2' in step:\n",
    "                    data = list(executor.map(DataProcessor.remove_short_fields, [data]))\n",
    "                    data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                if 'step3' in step:\n",
    "                    data = list(executor.map(DataProcessor.replace_sure_translation, [data]))\n",
    "                    data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                if 'step4' in step:\n",
    "                    data = list(executor.map(DataProcessor.delete_error_korean_prefix, [data]))\n",
    "                    data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                if 'step5' in step:\n",
    "                    data = list(executor.map(DataProcessor.do_not_translate_code_snippet, [data]))\n",
    "                    data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                if 'step6' in step:\n",
    "                    data = list(executor.map(DataProcessor.remove_duplicates, [data]))\n",
    "                    data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                if 'step7' in step:\n",
    "                    excluded_data = list(executor.map(DataProcessor.replace_output_prefix, [data]))\n",
    "                    excluded_data = DataProcessor.flatten_list(excluded_data)\n",
    "                    data = [d for d in data if d not in excluded_data]\n",
    "\n",
    "                if 'step8' in step:\n",
    "                    data = list(executor.map(DataProcessor.remove_deletion_and_addition, [data]))\n",
    "                    data = DataProcessor.flatten_list(data)\n",
    "\n",
    "        data = DataProcessor.flatten_list(data)\n",
    "        DataProcessor.write_to_file(data, output_file_path)\n",
    "        DataProcessor.write_to_file(excluded_data, dummy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_deletion_and_addition completed successfully in 8.89 seconds.\n",
      "write_to_file completed successfully in 14.55 seconds.\n",
      "write_to_file completed successfully in 0.00 seconds.\n",
      "process_json_file completed successfully in 23.67 seconds.\n"
     ]
    }
   ],
   "source": [
    "dp = DataProcessor('ko_shargpt_deepl_translate_cleaned_v1.json')\n",
    "output_file_path = 'ko_shargpt_deepl_translate_cleaned_v2.json'\n",
    "dummy_file_path = 'ko_shargpt_deepl_cleaned_dummy.json'\n",
    "# steps = ['step'+str(i) for i in range(9)]\n",
    "steps = ['step8']\n",
    "dp.process_json_file(steps, output_file_path, dummy_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miccai",
   "language": "python",
   "name": "miccai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
