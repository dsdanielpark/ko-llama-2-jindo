{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make 1 File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN -> KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "def extract_values(json_data_1, json_data_2):\n",
    "    values_list = []\n",
    "    \n",
    "    for item_1, item_2 in zip(json_data_1, json_data_2):\n",
    "        \n",
    "        conversations_1 = item_1.get(\"conversations\", [])\n",
    "        conversations_2 = item_2.get(\"conversations\", [])\n",
    "        \n",
    "        if len(conversations_1) != len(conversations_2):\n",
    "            continue\n",
    "        \n",
    "        for conversation_1, conversation_2 in zip(conversations_1, conversations_2):\n",
    "            value_1 = conversation_1.get(\"value\")\n",
    "            value_2 = conversation_2.get(\"value\")\n",
    "            \n",
    "            temp_dict = {} \n",
    "            \n",
    "            temp_dict['instruction'] = \"translate the following into korean\"\n",
    "            temp_dict['input'] = value_1\n",
    "            temp_dict['output'] = value_2\n",
    "            \n",
    "            if value_1 and value_2:\n",
    "                values_list.append(temp_dict)\n",
    "    \n",
    "    return values_list\n",
    "\n",
    "def process_json_files(file_path_1, file_path_2, chunk_size, output_filepath):\n",
    "    with open(file_path_1) as file:\n",
    "        data_1 = json.load(file)\n",
    "\n",
    "    with open(file_path_2) as file:\n",
    "        data_2 = json.load(file)\n",
    "\n",
    "    data_1_chunks = [data_1[i:i+chunk_size] for i in range(0, len(data_1), chunk_size)]\n",
    "    data_2_chunks = [data_2[i:i+chunk_size] for i in range(0, len(data_2), chunk_size)]\n",
    "\n",
    "    values = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for chunk_1, chunk_2 in zip(data_1_chunks, data_2_chunks):\n",
    "            future = executor.submit(extract_values, chunk_1, chunk_2)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            values.extend(future.result())\n",
    "\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(values, file, ensure_ascii=False,  indent=4, separators=(',', ':')) \n",
    "\n",
    "file_path_1 = r\"C:\\Users\\parkm\\Desktop\\llm\\ko-alpaca-lingo\\data\\dbdu\\ShareGPT-74k-ko\\original_shargpt.json\"\n",
    "file_path_2 = r\"C:\\Users\\parkm\\Desktop\\llm\\ko-alpaca-lingo\\data\\dbdu\\ShareGPT-74k-ko\\ko_sharegpt.json\"\n",
    "chunk_size = 9000000\n",
    "output_filepath = \"ko_shargpt_deepl_translate_cleaned_v1.json\"\n",
    "process_json_files(file_path_1, file_path_2, chunk_size, output_filepath)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KO -> EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "def extract_values(json_data_1, json_data_2, translate_type):\n",
    "    values_list = []\n",
    "    \n",
    "    if translate_type =='ko_to_en':\n",
    "        for item_1, item_2 in zip(json_data_1, json_data_2):\n",
    "            \n",
    "            conversations_1 = item_1.get(\"conversations\", [])\n",
    "            conversations_2 = item_2.get(\"conversations\", [])\n",
    "            \n",
    "            if len(conversations_1) != len(conversations_2):\n",
    "                continue\n",
    "            \n",
    "            for conversation_1, conversation_2 in zip(conversations_1, conversations_2):\n",
    "                value_1 = conversation_1.get(\"value\")\n",
    "                value_2 = conversation_2.get(\"value\")\n",
    "                \n",
    "                temp_dict = {} \n",
    "                \n",
    "                temp_dict['instruction'] = \"다음을 한국어로 번역해.\"\n",
    "                temp_dict['input'] = value_1\n",
    "                temp_dict['output'] = value_2\n",
    "                \n",
    "                if value_1 and value_2:\n",
    "                    values_list.append(temp_dict)\n",
    "    elif translate_type =='en_to_ko':\n",
    "        for item_1, item_2 in zip(json_data_1, json_data_2):\n",
    "            \n",
    "            conversations_1 = item_1.get(\"conversations\", [])\n",
    "            conversations_2 = item_2.get(\"conversations\", [])\n",
    "            \n",
    "            if len(conversations_1) != len(conversations_2):\n",
    "                continue\n",
    "            \n",
    "            for conversation_1, conversation_2 in zip(conversations_1, conversations_2):\n",
    "                value_1 = conversation_1.get(\"value\")\n",
    "                value_2 = conversation_2.get(\"value\")\n",
    "                \n",
    "                temp_dict = {} \n",
    "                \n",
    "                temp_dict['instruction'] = \"translate the following into English\"\n",
    "                temp_dict['input'] = value_1\n",
    "                temp_dict['output'] = value_2\n",
    "                \n",
    "                if value_1 and value_2:\n",
    "                    values_list.append(temp_dict)\n",
    "    \n",
    "    return values_list\n",
    "        \n",
    "    return values_list\n",
    "\n",
    "def process_json_files(file_path_1, file_path_2, chunk_size, output_filepath, translate_type):\n",
    "    with open(file_path_1) as file:\n",
    "        data_1 = json.load(file)[:200]\n",
    "\n",
    "    with open(file_path_2) as file:\n",
    "        data_2 = json.load(file)[:200]\n",
    "\n",
    "    data_1_chunks = [data_1[i:i+chunk_size] for i in range(0, len(data_1), chunk_size)]\n",
    "    data_2_chunks = [data_2[i:i+chunk_size] for i in range(0, len(data_2), chunk_size)]\n",
    "\n",
    "    values = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for chunk_1, chunk_2 in zip(data_1_chunks, data_2_chunks):\n",
    "            future = executor.submit(extract_values, chunk_1, chunk_2, translate_type)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            values.extend(future.result())\n",
    "\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(values, file, ensure_ascii=False,  indent=4, separators=(',', ':'))  \n",
    "\n",
    "file_path_2 = \"../data/original_dataset.json\"\n",
    "file_path_1 = \"../data/ko_dataset.json\"\n",
    "chunk_size = 1000\n",
    "output_filepath = \"ko_shargpt_deepl.json\"\n",
    "process_json_files(file_path_1, file_path_2, chunk_size, output_filepath, \"en_to_ko\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Save"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En -> Ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "def extract_values(json_data_1, json_data_2):\n",
    "    values_list = []\n",
    "    \n",
    "    for item_1, item_2 in zip(json_data_1, json_data_2):\n",
    "        \n",
    "        conversations_1 = item_1.get(\"conversations\", [])\n",
    "        conversations_2 = item_2.get(\"conversations\", [])\n",
    "        \n",
    "        if len(conversations_1) != len(conversations_2):\n",
    "            continue\n",
    "        \n",
    "        for conversation_1, conversation_2 in zip(conversations_1, conversations_2):\n",
    "            value_1 = conversation_1.get(\"value\")\n",
    "            value_2 = conversation_2.get(\"value\")\n",
    "            \n",
    "            temp_dict = {} \n",
    "            \n",
    "            temp_dict['instruction'] = \"Translate the following into English\"\n",
    "            temp_dict['input'] = value_1\n",
    "            temp_dict['output'] = value_2\n",
    "            \n",
    "            if value_1 and value_2:\n",
    "                values_list.append(temp_dict)\n",
    "    \n",
    "    return values_list\n",
    "\n",
    "def process_json_files(file_path_1, file_path_2, chunk_size, save_chunk_size, output_folder):\n",
    "    with open(file_path_1) as file:\n",
    "        data_1 = json.load(file)\n",
    "\n",
    "    with open(file_path_2) as file:\n",
    "        data_2 = json.load(file)\n",
    "\n",
    "    data_1_chunks = [data_1[i:i+chunk_size] for i in range(0, len(data_1), chunk_size)]\n",
    "    data_2_chunks = [data_2[i:i+chunk_size] for i in range(0, len(data_2), chunk_size)]\n",
    "\n",
    "    values = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for chunk_1, chunk_2 in zip(data_1_chunks, data_2_chunks):\n",
    "            future = executor.submit(extract_values, chunk_1, chunk_2)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            values.extend(future.result())\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    chunked_values = [values[i:i+save_chunk_size] for i in range(0, len(values), save_chunk_size)]\n",
    "    for i, chunked_value in enumerate(chunked_values):\n",
    "        filename = f\"{output_folder}/output_{i+1}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(chunked_value, file, ensure_ascii=False,  indent=4, separators=(',', ':'))\n",
    "\n",
    "file_path_1 = \"../data/original_dataset.json\" # From Language\n",
    "file_path_2 = \"../data/ko_dataset.json\"  # To Language\n",
    "chunk_size = 1000\n",
    "save_chunk_size = 20000\n",
    "output_folder = \"output\"\n",
    "process_json_files(file_path_1, file_path_2, chunk_size, save_chunk_size, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miccai",
   "language": "python",
   "name": "miccai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
