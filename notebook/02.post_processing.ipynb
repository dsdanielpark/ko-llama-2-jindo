{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, input_file_path):\n",
    "        with open(input_file_path) as f:\n",
    "            self.data = json.load(f)\n",
    "        self.excluded_data = []\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_specific_words(data):\n",
    "        word_set = {'sure!', 'great!', 'Certainly!', \"Sure!\", \"Great!\", \"Great, \", \"great, \",\n",
    "                    \"sure, \", \"Sure, \", \"Sure! \"}\n",
    "        return [d for d in data if not any(word in d['input'] for word in word_set)]\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_short_fields(data):\n",
    "        try:\n",
    "            return [d for d in data if (input_len := len(d['input'])) > 4 and (output_len := len(d['output'])) > 4]\n",
    "        except Exception as e:\n",
    "            print(f\"Error at remove_short_fields {e} \\n Finished unsuccessfully.\")\n",
    "            return data\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_sure_translation(data):\n",
    "        for d in data:\n",
    "            try:\n",
    "                d['output'] = re.sub(r'\\b물론,\\b', '물론이죠.', d['output'])\n",
    "                d['output'] = re.sub(r'\\b확실히,\\b', '', d['output'])\n",
    "                d['output'] = re.sub(r'\\b예,\\b', '네.', d['output'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error at replace_sure_translation {e}\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_error_korean_prefix(data):\n",
    "        for d in data:\n",
    "            try:\n",
    "                d['output'] = re.sub(r'^(은 |는 )', '', d['output'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error at delete_error_korean_prefix {e}\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_output_prefix(data):\n",
    "        prefix_set = {\"을 \", \"를 \", \"이 \", \"가 \", \"h\", \"은 \", \"는 \", \"에 \", \"으 \", \"의\", \"예, \", \"^[A-Za-z] \", \"^[ㄱ-ㅎㅏ-ㅣ가-힣] \", \"^[0-9] \", \".\", \",\"}\n",
    "        exclued_data = []\n",
    "\n",
    "        for d in data:\n",
    "            try:\n",
    "                output_text = d['output']\n",
    "                if output_text.startswith(tuple(prefix_set)):\n",
    "                    exclued_data.append(d)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at replace_output_prefix {e}\")\n",
    "            \n",
    "        return exclued_data\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def do_not_translate_code_snippet(data):\n",
    "        for d in data:\n",
    "            try:\n",
    "                input_text = d['input']\n",
    "                output_text = d['output']\n",
    "\n",
    "                if '```' in input_text and '```' in output_text:\n",
    "                    start_index = input_text.find('```') + 3\n",
    "                    end_index = input_text.find('```', start_index)\n",
    "                    replace_text = input_text[start_index:end_index]\n",
    "\n",
    "                    d['output'] = output_text.replace('```', f' ```{replace_text}```')\n",
    "            except Exception as e:\n",
    "                print(f\"Error at do_not_translate_code_snippet {e}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_duplicates(data):\n",
    "        unique_data = []\n",
    "        seen_inputs = set()\n",
    "        seen_outputs = set()\n",
    "        for d in data:\n",
    "            input_value = d[\"input\"]\n",
    "            output_value = d[\"output\"]\n",
    "            if (input_value, output_value) not in seen_inputs and \\\n",
    "                    (input_value, output_value) not in seen_outputs and \\\n",
    "                    input_value != output_value:\n",
    "                seen_inputs.add((input_value, output_value))\n",
    "                seen_outputs.add((input_value, output_value))\n",
    "                unique_data.append(d)\n",
    "        return unique_data\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_deletion_and_addition(data):\n",
    "        for d in data:\n",
    "            input_value = d[\"input\"]\n",
    "            output_value = d[\"output\"]\n",
    "\n",
    "            input_words = input_value.split()\n",
    "            output_words = output_value.split()\n",
    "\n",
    "            if len(output_words[0]) > 1 and len(input_words[0]) > 2:\n",
    "                if len(set(output_words[0].lower()) - set(input_words[0].lower())) < 2 and \\\n",
    "                        input_words[0][1].lower() == output_words[0][0].lower() and \\\n",
    "                        input_words[0][2].lower() == output_words[0][1].lower():\n",
    "                    output_words[0] = input_words[0]\n",
    "                    output_value = \" \".join(output_words)\n",
    "                    d[\"output\"] = output_value\n",
    "\n",
    "            if output_words[0] == \"물론,\":\n",
    "                output_words[0] = \"물론이죠. \"\n",
    "                output_value = \" \".join(output_words)\n",
    "                d[\"output\"] = output_value\n",
    "\n",
    "            if len(output_words[0]) == 1 and output_words[0].isalpha() and output_words[0].isascii() and output_words[0].lower() != \"a\":\n",
    "                output_words[0] = \"\"\n",
    "                output_value = \" \".join(output_words)\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_list(data):\n",
    "        flattened_list = []\n",
    "        for sublist in data:\n",
    "            if isinstance(sublist, list):\n",
    "                flattened_list.extend(DataProcessor.flatten_list(sublist))\n",
    "            else:\n",
    "                flattened_list.append(sublist)\n",
    "        return flattened_list\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_file(data, file_path):\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "\n",
    "\n",
    "    def process_json_file(self, steps, output_file_path, dummy_file_path):\n",
    "        for step in steps:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                if 'step1' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_specific_words, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step2' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_short_fields, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step3' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.replace_sure_translation, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step4' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.delete_error_korean_prefix, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step5' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.do_not_translate_code_snippet, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step6' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_duplicates, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "                if 'step7' in step:\n",
    "                    self.excluded_data = list(executor.map(self.replace_output_prefix, [self.data]))\n",
    "                    self.excluded_data = DataProcessor.flatten_list(self.excluded_data)\n",
    "                    self.data = [d for d in self.data if d not in self.excluded_data]\n",
    "\n",
    "                if 'step8' in step:\n",
    "                    self.data = list(executor.map(DataProcessor.remove_deletion_and_addition, [self.data]))\n",
    "                    self.data = DataProcessor.flatten_list(self.data)\n",
    "\n",
    "        self.data = DataProcessor.flatten_list(self.data)\n",
    "        self.write_to_file(self.data, output_file_path)\n",
    "        self.write_to_file(self.excluded_data, dummy_file_path)\n",
    "\n",
    "\n",
    "        def static_process_json_file(steps, input_file_path, output_file_path, dummy_file_path):\n",
    "            with open(input_file_path) as f:\n",
    "                data = json.load(f)\n",
    "            excluded_data = []\n",
    "            \n",
    "            for step in steps:\n",
    "                with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                    if 'step1' in step:\n",
    "                        data = list(executor.map(DataProcessor.remove_specific_words, [data]))\n",
    "                        data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                    if 'step2' in step:\n",
    "                        data = list(executor.map(DataProcessor.remove_short_fields, [data]))\n",
    "                        data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                    if 'step3' in step:\n",
    "                        data = list(executor.map(DataProcessor.replace_sure_translation, [data]))\n",
    "                        data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                    if 'step4' in step:\n",
    "                        data = list(executor.map(DataProcessor.delete_error_korean_prefix, [data]))\n",
    "                        data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                    if 'step5' in step:\n",
    "                        data = list(executor.map(DataProcessor.do_not_translate_code_snippet, [data]))\n",
    "                        data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                    if 'step6' in step:\n",
    "                        data = list(executor.map(DataProcessor.remove_duplicates, [data]))\n",
    "                        data = DataProcessor.flatten_list(data)\n",
    "\n",
    "                    if 'step7' in step:\n",
    "                        excluded_data = list(executor.map(replace_output_prefix, [data]))\n",
    "                        excluded_data = DataProcessor.flatten_list(excluded_data)\n",
    "                        data = [d for d in data if d not in excluded_data]\n",
    "\n",
    "                    if 'step8' in step:\n",
    "                        data = list(executor.map(DataProcessor.remove_deletion_and_addition, [data]))\n",
    "                        data = DataProcessor.flatten_list(data)\n",
    "\n",
    "            data = DataProcessor.flatten_list(data)\n",
    "            write_to_file(data, output_file_path)\n",
    "            write_to_file(excluded_data, dummy_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataProcessor('ko_shargpt_deepl_translate_cleaned_v1.json')\n",
    "output_file_path = 'ko_shargpt_deepl_translate_cleaned_v1.json'\n",
    "dummy_file_path = 'ko_shargpt_deepl_cleaned_dummy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['step'+str(i) for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.process_json_file(steps, output_file_path, dummy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"danielpark/llama-tokenizer\")\n",
    "tokenizer.encode(\"Hi, I'm Minwoo Park from seoul, korea.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miccai",
   "language": "python",
   "name": "miccai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
